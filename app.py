# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_4sJCrotstD_XW1v4sJjunj_WqiYTOx
"""

# app.py
import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re

# Helper functions for each website
def scrape_cspi():
    URL = "https://www.cspi.org/page/media"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    page = requests.get(URL, headers=headers)
    articles_data = []

    if page.status_code != 403:
        soup = BeautifulSoup(page.content, "html.parser")
        articles = soup.find_all('div', class_='teaser-inner')
        two_weeks_ago = datetime.now() - timedelta(weeks=2)

        for a in articles:
            title_element = a.find("a")
            date_element = a.find("time")
            link_element = a.find("a", class_="js-link-event-link")
            label_element = a.find("span", class_="source")

            if date_element:
                try:
                    article_date = datetime.strptime(date_element.text.strip(), "%B %d, %Y")
                except:
                    continue
                if article_date >= two_weeks_ago:
                    articles_data.append({
                        "title": title_element.text.strip() if title_element else "Title not found",
                        "topic": label_element.text.strip() if label_element else "Topic not found",
                        "date": date_element.text.strip(),
                        "date_obj": article_date,
                        "link": link_element['href'] if link_element else "Link not found",
                        "source": "Center for Science in the Public Interest"
                    })
    return articles_data

def scrape_mighty_earth():
    URL = "https://mightyearth.org/news/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    page = requests.get(URL, headers=headers)
    articles_data = []

    if page.status_code != 403:
        soup = BeautifulSoup(page.content, "html.parser")
        articles = soup.find_all('div', class_='card card-article uk-transition-toggle reveal')
        two_weeks_ago = datetime.now() - timedelta(weeks=2)

        for a in articles:
            title_element = a.find("h5")
            topic_element = a.find("label")
            date_element = a.find("span", class_="date")
            link_element = a.find("a", class_="card-link")

            if date_element:
                try:
                    article_date = datetime.strptime(date_element.text.strip(), "%d/%b/%Y")
                except:
                    continue
                if article_date >= two_weeks_ago:
                    formatted_date = article_date.strftime("%b %d, %Y")
                    articles_data.append({
                        "title": title_element.text.strip() if title_element else "Title not found",
                        "topic": topic_element.text.strip() if topic_element else "Topic not found",
                        "date": formatted_date,
                        "date_obj": article_date,
                        "link": link_element['href'] if link_element else "Link not found",
                        "source": "Mighty Earth"
                    })
    return articles_data

def scrape_cfs():
    URL = "https://www.centerforfoodsafety.org/press-releases"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    page = requests.get(URL, headers=headers)
    articles_data = []

    if page.status_code != 403:
        soup = BeautifulSoup(page.content, "html.parser")
        articles = soup.find_all('div', class_='no_a_color padB2')
        two_weeks_ago = datetime.now() - timedelta(weeks=2)

        for a in articles:
            title_element = a.find(class_ = "padB1 txt_17 normal txt_red")
            date_element = a.find(class_="txt_12 iblock padB0")
            link_element = a.find("a")

            if date_element:
                try:
                    clean_date = re.sub(r'(\d+)(st|nd|rd|th)', r'\1', date_element.text)
                    article_date = datetime.strptime(clean_date, "%B %d, %Y")
                except:
                    continue
                if article_date >= two_weeks_ago:
                    formatted_date = article_date.strftime("%b %d, %Y")
                    articles_data.append({
                        "title": title_element.text.strip() if title_element else "Title not found",
                        "topic": "Topic not found",
                        "date": formatted_date,
                        "date_obj": article_date,
                        "link": "https://www.centerforfoodsafety.org" + link_element['href'] if link_element else "Link not found",
                        "source": "Center for Food Safety"
                    })
    return articles_data

# Streamlit UI
st.set_page_config(page_title="Environmental News Aggregator", layout="wide")
st.markdown("<h1 style='font-size: 30px;'>ðŸ“… Latest Articles from Selected Websites</h1>", unsafe_allow_html=True)
st.markdown("<p style='font-size: 18px;'>Select the sources you want to search:</p>", unsafe_allow_html=True)

# User selections
show_cspi = st.checkbox("Center for Science in the Public Interest")
show_mighty = st.checkbox("Mighty Earth")
show_cfs = st.checkbox("Center for Food Safety")

if st.button("Search"):
    all_articles = []
    if show_cspi:
        all_articles += scrape_cspi()
    if show_mighty:
        all_articles += scrape_mighty_earth()
    if show_cfs:
        all_articles += scrape_cfs()

    # Sort articles by date (most recent first)
    all_articles.sort(key=lambda x: x['date_obj'], reverse=True)

    if all_articles:
        for article in all_articles:
            with st.container():
                st.markdown("""
                    <div style='background-color:#f9f9f9;padding:20px;border-radius:10px;margin-bottom:20px;box-shadow:0 4px 8px rgba(0, 0, 0, 0.05);'>
                        <h3 style='font-size:18px;margin-bottom:10px;'>
                            <a href='{link}' target='_blank' style='text-decoration:none;color:#1a73e8;'>{title}</a>
                        </h3>
                        <p style='font-size:12px;margin:0;'><strong>Topic:</strong> {topic}</p>
                        <p style='font-size:12px;margin:0;'><strong>Date:</strong> {date}</p>
                        <p style='font-size:12px;margin:0;'><strong>Source:</strong> {source}</p>
                    </div>
                """.format(
                    title=article['title'],
                    link=article['link'],
                    topic=article['topic'],
                    date=article['date'],
                    source=article['source']
                ), unsafe_allow_html=True)
    else:
        st.info("No articles found in the past two weeks from the selected sources.")
